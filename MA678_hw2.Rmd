---
title: "Homework 02_MA678"
author: "Yifu Dong"
date: "Septemeber 23, 2018"
output:
  pdf_document: default
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
pacman::p_load("arm","data.table","Cairo","faraway","foreign","ggplot2","alr3","knitr")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Introduction 
In homework 2 you will fit many regression models.  You are welcome to explore beyond what the question is asking you.  

Please come see us we are here to help.

# Data analysis 

## Analysis of earnings and height data

The folder `earnings` has data from the Work, Family, and Well-Being Survey (Ross, 1990).
You can find the codebook at http://www.stat.columbia.edu/~gelman/arm/examples/earnings/wfwcodebook.txt
```{r}
gelman_dir <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
heights    <- read.dta (paste0(gelman_dir,"earnings/heights.dta"))
```

Pull out the data on earnings, sex, height, and weight.


###1. In R, check the dataset and clean any unusually coded data.

```{r}
View(heights)#check the dataset
```
We can find many missing data which is not applicable in this dataset. 
We don't know the true value of these missing data, and don't know whether these data is 0 or empty, so we'd better to remove missing records from the data. Also, we are also supposed to scale earnings so that the outliers have less influence.

Also, we can scale earnings since it's easier to interpret using thousand money as our unit. Moreover, it can be more centered.
```{r}
#remove na
heights_clean <-na.omit(heights) 

#scale earnings 
heights_clean$earn <- heights_clean$earn/1000

# remove observations where yearbn > 90
heights_clean <- heights_clean[heights_clean$yearbn <= 90,]

summary(heights_clean)
```




###2. Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model
as average earnings for people with average height?

In order to interpret the intercept as average earnings for people with average height, we can normoalize earning and height.
```{r}
height <- (heights_clean$height- mean(heights_clean$height))/(2*sd(heights_clean$height))
earning <- (heights_clean$earn- mean(heights_clean$earn))/(2*sd(heights_clean$earn))
regout1_2 <- lm(earning~height)
summary(regout1_2)
```




###3. Fit some regression models with the goal of predicting earnings from some
combination of sex, height, and yearbn. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.

```{r}
# create variable `age`
age <- 90 - heights_clean$yearbn
#transformation
age <- (age-mean(age))/(2*sd(age))
regout1_3 <- lm(earning~heights_clean$sex+heights_clean$sex*height+heights_clean$sex*age)
summary(regout1_3)
```
We fit the model with 3 predictors, sex, height, and yearbn.  For yearbn, we transfer it into people's age, and then normalize it. And we normalize all there 3 predictors.
In this model we interact sex with next 2 predictors since we don't know whether sex has an influence on height and yearbn. Luckily it turns out that this model fits a little better than model without interaction, and the p value of sex::age is significant. 


###4. Interpret all model coefficients.

Intercept: the intercept represent the average income for a male of average age and height.
Sex: female who have average age and height, earn $2700 less then males with similar characteristic. 
Age: individuals who are 1 standard deviation from the average age earn $8200 
Height: individuals who are 1 standard deviation from the average height earn $7000
heights_clean$sex:age : Women earn $1800 less than male individuals with same age.

###5. Construct 95% confidence interval for all model coefficients and discuss what they mean.

```{r}
confint(regout1_3,'heights_clean$sex',level = 0.95)
confint(regout1_3,'height',level = 0.95)
confint(regout1_3,'age',level=0.95)
confint(regout1_3,'heights_clean$sex:height',level = 0.95)
confint(regout1_3, 'heights_clean$sex:age',level=0.95)
```
This confidence intervals mean that if we fit the model over and over again, then 95% coefficients 
will be in this intervals.


## Analysis of mortality rates and various environmental factors

The folder `pollution` contains mortality rates and various environmental factors from 60 U.S. metropolitan areas from McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482. 

Variables, in order:

* PREC   Average annual precipitation in inches
* JANT   Average January temperature in degrees F
* JULT   Same for July
* OVR65  % of 1960 SMSA population aged 65 or older
* POPN   Average household size
* EDUC   Median school years completed by those over 22
* HOUS   % of housing units which are sound & with all facilities
* DENS   Population per sq. mile in urbanized areas, 1960
* NONW   % non-white population in urbanized areas, 1960
* WWDRK  % employed in white collar occupations
* POOR   % of families with income < $3000
* HC     Relative hydrocarbon pollution potential
* NOX    Same for nitric oxides
* SO@    Same for sulphur dioxide
* HUMID  Annual average % relative humidity at 1pm
* MORT   Total age-adjusted mortality rate per 100,000

For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
```

###1. Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
plot(pollution$nox,pollution$mort)
```
We can know from the figure above that linear regression modelmay be a good model for there data.

```{r}
regout2 <- lm(pollution$mort~pollution$nox, data=pollution)
display(regout2)
```

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
pollution_clean <- pollution
par(mfrow=c(2,2))
plot(regout2)
plot(pollution$nox,pollution$mort)
regout2 <- lm(pollution$mort~pollution$nox, data=pollution)
abline(regout2)
#overall fit
marginalModelPlots(regout2,col=rgb(0,0,0,alpha=0.3),col.line = c("green","red"))
```


From the plots of residuals and the relation between this two variables, we cannot say it fits very well. Residuals suffer from heteroschedasticity.**It seems that outliers exist.But we cannot remove them straightly.**



###2. Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

Actually, the R^square of the model above is only 0.01, which means the relation of two variables is posibly not linear. So we use log to see what happens. 
```{r}
regout2_2 <- lm(log(pollution_clean$mort) ~ (pollution_clean$nox), data=pollution_clean)
display(regout2_2)
par(mfrow=c(2,2))
plot(regout2_2)
```
The R-Squared is still 0.01, without and improvement.



```{r}
regout2_2 <- lm(log(pollution_clean$mort) ~ log(pollution_clean$nox), data=pollution_clean)
display(regout2_2)
par(mfrow=c(2,2))
plot(regout2_2)
```

From the figures above, the log model fits better, but residuals still suffer from heteroschedasticity. 
Then we try to regress log(nox) on mort, the result is worse. Now we try another model:
```{r}
regout2_2 <- lm(log(pollution_clean$mort) ~ log(pollution_clean$nox)+pollution_clean$nox, data=pollution_clean)
display(regout2_2)
par(mfrow=c(2,2))
plot(regout2_2)
```
This model fits much better, although it's still not significant enough for this two variables.

Now we normalize the variables, however,when we normalize the data, it's hard to use log transformation since NaNs exist easier. So we don't choose to normalize data.


```{r}
residualPlots(regout2_2, terms= ~ 1, fitted=TRUE)
```

Tukey test is not significant, which means that this model is actually still not good enough.

Residuals still suffer from heteroschedasticity.



###3. Interpret the slope coefficient from the model you chose in 2.}

Intercept: The average morality rate when NO equals 0 is $exp(6.77) =871.3119 $
log(nox):For each 1 difference in nitric oxide , the predicted difference in morality rate is +0.04%


###4. Construct 99% confidence interval for slope coefficient from the model you chose in 2 and interpret them.

```{r}
confint(regout2_2,'log(pollution_clean$nox)',level = 0.99)
```
This means that if we fit the model and calculate the slope over and over again, 99% true value of the slope coefficient will be in the interval (0.01378255, 0.06240595)



###5. Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
#normalize
so2n<- (pollution_clean$so2 - mean(pollution_clean$so2)) / (2*sd(pollution_clean$so2))
hcn<- (pollution_clean$hc - mean(pollution_clean$hc)) / (2*sd(pollution_clean$hc))

#regression
regout2_5 <- lm(log(pollution_clean$mort)~log(pollution_clean$nox)+so2n+hcn, data=pollution_clean)
display(regout2_5)
par(mfrow=c(2,2))
plot(regout2_5)

```


**Before the model above, we tried not to normalize the predictors and tried to add "log" relatively. Finally we found that the model above fits best. So we choose this model.**

Interpretation: 

Intercept: The mortality rate for an individual exposed to average levels of nitric oxides, sulfur dioxide, and hydrocarbons is $exp(6.73) = 837.1473$

log(pollution_clean$nox): 1 standard deviation difference for nitric oxides corresponds to a mortality rate 5% higher.

so2n: 1 standard deviation difference for sulfur dioxide corresponds to $exp(0.03)=1.030455$ increase in mortality rate.

hcn: 1 standard deviation difference in hydrocarbons corresponds to a mortality rate $exp(-0.10) = 0.948374$ times lower, which is a decrease of about 6%.


###6. Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in 4, so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)

```{r}
#divide the dataset into 2 part: train dataset and predict dataset
train <-  pollution_clean[1:(nrow(pollution_clean)/2),]
pred <- pollution_clean[((nrow(pollution_clean)/2)+1):nrow(pollution_clean),]

#normalize choosing the data from training dataset.
so2n<- (train$so2 - mean(train$so2)) / (2*sd(train$so2))
hcn<- (train$hc - mean(train$hc)) / (2*sd(train$hc))
regout2_6 <- lm(log(train$mort)~log(train$nox)+so2n+hcn, data=train)
display(regout2_6)

```

```{r}
#predict
predictions <- predict(regout2_6, pred)
cbind(predictions=exp(predictions), observed=pred$mort)
plot(exp(predictions), pred$mort)
abline(a=0, b=1)
```

```{r}
# compute RMSE
sqrt(mean((pred$mort-exp(predictions))^2))
#compute R Squared
summary(regout2_6)["r.squared"] 
```






## Study of teenage gambling in Britain

```{r,message =FALSE}
data(teengamb)
?teengamb
```

###1. Fit a linear regression model with gamble as the response and the other variables as predictors and interpret the coefficients. Make sure you rename and transform the variables to improve the interpretability of your regression model.


Notice that the scale of these four predictors are totally different, so first we standardize these predictors by subtracting the mean and dividing by 2 standard deviations
```{r}
gamble <- (teengamb$gamble - mean(teengamb$gamble)) / (2*sd(teengamb$gamble))
sex <- (teengamb$sex - mean(teengamb$sex)) / (2*sd(teengamb$sex))
status <- (teengamb$status - mean(teengamb$status)) / (2*sd(teengamb$status))
income <- (teengamb$income - mean(teengamb$income)) / (2*sd(teengamb$income))
verbal <- (teengamb$verbal - mean(teengamb$verbal)) / (2*sd(teengamb$verbal))
regout3_1 <- lm(gamble~sex+status+income+verbal,data=teengamb)
summary(regout3_1)
```
By standarlization, we improve the interpretability of your regression model.
Notice that standarlized predictors contains 0, and other numbers which are not able to do logarithmic transformation.
But we can still improve the interpretability by adding interactions.
```{r}
gamble <- (teengamb$gamble - mean(teengamb$gamble)) / (2*sd(teengamb$gamble))
sex <- (teengamb$sex - mean(teengamb$sex)) / (2*sd(teengamb$sex))
status <- (teengamb$status - mean(teengamb$status)) / (2*sd(teengamb$status))
income <- (teengamb$income - mean(teengamb$income)) / (2*sd(teengamb$income))
verbal <- (teengamb$verbal - mean(teengamb$verbal)) / (2*sd(teengamb$verbal))
regout3_2 <- lm(gamble~sex+status+income+verbal+sex*income+sex*status+sex*verbal,data=teengamb)
summary(regout3_2)
```
Notice the R-squared is 0.62, so this model which contains interaction is better.


###2. Create a 95% confidence interval for each of the estimated coefficients and discuss how you would interpret this uncertainty.

```{r}
confint(regout3_1,'sex',level = 0.99)
confint(regout3_1,'status',level = 0.99)
confint(regout3_1,'income',level = 0.99)
confint(regout3_1,'verbal',level = 0.99)

```
Interpretation: The true value of these coefficients have the possibility of 99% to be in this confidence interval. 
That means, if we fit the model using the related data over and over again, 99% estimated coefficients will be in our confidence interval.



###3. Predict the amount that a male with average status, income and verbal score would gamble along with an appropriate 95% CI.  Repeat the prediction for a male with maximal values of status, income and verbal score.  Which CI is wider and why is this result expected?

```{r}
gamble <- (teengamb$gamble - mean(teengamb$gamble)) / (2*sd(teengamb$gamble))
sex <- (teengamb$sex - mean(teengamb$sex)) / (2*sd(teengamb$sex))
status <- (teengamb$status - mean(teengamb$status)) / (2*sd(teengamb$status))
income <- (teengamb$income - mean(teengamb$income)) / (2*sd(teengamb$income))
verbal <- (teengamb$verbal - mean(teengamb$verbal)) / (2*sd(teengamb$verbal))
#regression
regout3_3 <- lm(gamble~sex+status+income+verbal)
regout3_3
#average status classfied by sex
avgstatus <- tapply(teengamb$status,teengamb$sex,FUN = mean)
#average income classfied by sex
avgincome <- tapply(teengamb$income,teengamb$sex,FUN = mean)
#average verbal classfied by sex
avgverbal <- tapply(teengamb$verbal,teengamb$sex,FUN = mean)

#dataframe for predict()
newdata <- data.frame(sex=1,status=avgstatus[2],income=avgincome[2],verbal=avgverbal[2])
predict(regout3_3,newdata,interval="confidence")
```
For this question, the CI is (-8.855479,12.57845)

```{r}
#For the seconde question, repeat the prediction for a male with maximal values of status, income and verbal score.
#max status classfied by sex
maxstatus <- tapply(teengamb$status,teengamb$sex,FUN = max)
#max income classfied by sex
maxincome <- tapply(teengamb$income,teengamb$sex,FUN = max)
#max verbal classfied by sex
maxverbal <- tapply(teengamb$verbal,teengamb$sex,FUN = max)
#dataframe for predict()
newdata1 <- data.frame(sex=1,status=maxstatus[2],income=maxincome[2],verbal=maxverbal[2])
predict(regout3_3,newdata1,interval="confidence")
```
For maximum, the CI is (-14,567,25.98372).

Obviously, the CI with maximal values is wider. Since the maximum data values is greater than the gamble amount for average data values, hence the standard error and the width of confidence interval is wider.





## School expenditure and test scores from USA in 1994-95

```{r}
data(sat)
?sat
```

###1. Fit a model with total sat score as the outcome and expend, ratio and salary as predictors. Make necessary transformation in order to improve the interpretability of the model.  Interpret each of the coefficient.

```{r}
regout4_1 <- lm(sat$total~sat$expend+sat$ratio+sat$salary,data=sat)
summary(regout4_1)
##scaling using logrithmic transformation to improve the interpretability.
ratio <- sat$ratio/100
regout4_2 <- lm(log(sat$total)~sat$expend+ratio+sat$salary,data=sat)
summary(regout4_2)
```
Interpretation: 
expend:  1 standard deviation difference for expend corresponds to a 1.7% higher in total.
ratio: 1 standard deviation difference for ratio(without%) corresponds to 0.6% increase in total.
salary: 1 standard deviation difference for salary(thousand dollars) corresponds to -0.9% increase in total.


###2. Construct 98% CI for each coefficient and discuss what you see.

```{r}
confint(regout4_1,'sat$expend',level = 0.98)
confint(regout4_1,'ratio',level = 0.98)
confint(regout4_1,'sat$salary',level = 0.98)
```
From each coefficient we can notice that we cannot determine whether each coefficient is negative or positive. 


###3. Now add takers to the model.  Compare the fitted model to the previous model and discuss which of the model seem to explain the outcome better?

```{r}
regout4_3 <- lm(sat$total~sat$expend+sat$ratio+sat$salary+sat$takers,data = sat)
summary(regout4_3)
```
Multiple R-squared is 0.8246,  and p value for takers is 2.61e-16, so obviously this model fits better than previous model.




# Conceptual exercises.

## Special-purpose transformations:

For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.

Discuss the advantages and disadvantages of the following measures:

###* The simple difference, $D_i-R_i$

Advantage: It is a good measure, since it is symmetric and centered. 

Disadvantage: It will not show the proportion of the amount of money. 3 million minus 2 million is 1 million, but 300 million minus 299 million is also 1 million. When measured by in this way, we won't know the proportion.


###* The ratio, $D_i/R_i$

Advantage: this measure is proportional, we can easily find the difference of proportion of the money.

Disadvantage : It is not symmetric. When the money of two parties is equal, the ratio is 1. If D/R=2, then the variable equals 2, but if R/D=2 the variable equals 0.5, so when using this variable for regression, it's hard to find a linear relation. 


###* The difference on the logarithmic scale, $log D_i-log R_i$ 

It is symmetric and centered. Also, this model will less influenced by outliers and heteroscedasticity. And this measure is proportional to the magnitude.So a 1M difference between the parties on a county where each raise on average above 10M will have a lower value than the same difference on a district where foundraising is poorer


###* The relative proportion, $D_i/(D_i+R_i)$.


Advantage: Since $D_i/(D_i+R_i)=1-R_i/(D_i+R_i)$, so it is symmetric and centered at 0.5.

Disadvantage: It is not proportional. The same difference on D and R will cause different change on this measure.


## Transformation 

For observed pair of $\mathrm{x}$ and $\mathrm{y}$, we fit a simple regression model 
$$\mathrm{y}=\alpha + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
which results in estimates $\hat{\alpha}=1$, $\hat{\beta}=0.9$, $SE(\hat{\beta})=0.03$, $\hat{\sigma}=2$ and $r=0.3$.

###1. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=\mathrm{x}-10$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star}$, $\hat{\beta}^{\star}$, $\hat{\sigma}^{\star}$, and $r^{\star}$.  What happens to these quantities when $\mathrm{x}^{\star}=10\mathrm{x}$ ? When $\mathrm{x}^{\star}=10(\mathrm{x}-1)$?

(i):Let $x*=x-10$ , so $x=x*+10$. So we have the model: 
 $$\mathrm{y}=\alpha + \beta \mathrm{(x^*+10)} + \mathrm{\epsilon}=(\alpha+10*\beta)+\beta x^*+\mathrm{\epsilon}$$ 
So for $x^*$, the value of $\beta$,$\sigma$,$\alpha$,$r$ are:$$\hat{\beta}^{\star}=\beta=0.9$$
$$\hat{\alpha}^{\star}=\alpha+10*0.9=1+9=10$$
$$\hat{\sigma}^{\star}=\sigma=2$$
$$r^{\star}=r=0.3$$

(ii):Let $x*=10x$ , so $x=x/10$. So we have the model:
So for $x^*$, the value of $\alpha$ doesn't change, $\hat{\alpha}^*=\hat{\alpha}=1$
$$\hat{\beta}^{\star}=\frac{\beta}{10}=0.09$$
 $$\hat{\sigma}^{\star}=\sigma*10=20$$
$$r^{\star}=r=0.3$$
 
 
(iii):Let $\mathrm{x}^{\star}=10(\mathrm{x}-1)$, so $x=(x^*/10)+1$. Hence we have the model:
$$\mathrm{y}=\alpha + \beta \mathrm{((x^*/10)+1)} + \mathrm{\epsilon}=\alpha+\beta+\frac{\beta}{10} x^*+\mathrm{\epsilon}$$ 
so $$\hat{\beta}^{\star}=\frac{\beta}{10}=0.09$$
$$\hat{\alpha}^{\star}=\alpha+\beta=1+0.9=1.9$$
$$\hat{\sigma}^{\star}=\sigma*10=20$$
$$r^{\star}=r=0.3$$ 



###2. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}= \mathrm{y}+10$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star\star}$, $\hat{\beta}^{\star\star}$, $\hat{\sigma}^{\star\star}$, and $r^{\star\star}$.  What happens to these quantities when $\mathrm{y}^{\star\star}=5\mathrm{y}$ ? When $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$?

(i): Let $\mathrm{y}^{\star\star}=y+10$ , so $y=\mathrm{y}^{\star\star}-10$. So we have the model: 
 $$\mathrm{y}^{\star\star}=\alpha+10 + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
so  $$\hat{\alpha}^{\star\star}=\alpha+10=1+10=11$$
$$\hat{\beta}^{\star\star}=\beta=0.9$$
$$\hat{\sigma}^{\star\star}=\sigma=2$$
$$r^{\star\star}=r=0.3$$


(ii): Let $\mathrm{y}^{\star\star}=5\mathrm{y}$,so $\mathrm{y}=\mathrm{y}^{\star\star}/5$. Hence we have the model:
 $$\mathrm{y}^{\star\star}/5=\alpha+ \beta \mathrm{x} + \mathrm{\epsilon}$$ 
 So
$$\mathrm{y}^{\star\star}=5\alpha+ 5\beta \mathrm{x} + 5\mathrm{\epsilon}$$ 
 $$\hat{\alpha}^{\star\star}=5\alpha=5$$
 $$\hat{\beta}^{\star\star}=5\beta=5*0.9=4.5$$
 $$\hat{\sigma}^{\star\star}=5\sigma=10$$
 $$r^{\star\star}=r=0.3$$
 
 




(iii):Let $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)=5\mathrm{y}+10$,so $\mathrm{y}=(\mathrm{y}^{\star\star}-10)/5$. Hence we have the model:
 $$\frac{\mathrm{y}^{\star\star}-10}{5}=\alpha+ \beta \mathrm{x} + \mathrm{\epsilon}$$ 
 Hence, $$\mathrm{y}^{\star\star}=(5\alpha+10)+5 \beta \mathrm{x} + 5\mathrm{\epsilon}$$
 $$\hat{\alpha}^{\star\star}=5\alpha+10=15$$
 $$\hat{\beta}^{\star\star}=5\beta=5*0.9=4.5$$
 $$\hat{\sigma}^{\star\star}=5\sigma=10$$
 $$r^{\star\star}=r=0.3$$
 




###3. In general, how are the results of a simple regression analysis affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?

Linear transformations do not affect the fit of a classical regression model, and they do not affect predictions: the changes in the inputs and the coefficients cancel in forming the predicted value. However, well-chosen linear transformation can improve interpretability of coefficients and make a fitted model easier to understand.



###4. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=10(\mathrm{x}-1)$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star})$ and $t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})$.

Let $\mathrm{x}^{\star}=10(\mathrm{x}-1)$,so $x=(x^*/10)+1$. Hence we have the model:
$$\mathrm{y}=\alpha + \beta \mathrm{((x^*/10)+1)} + \mathrm{\epsilon}=\alpha+\beta+\frac{\beta}{10} x^*+\mathrm{\epsilon}$$ 
so $$\hat{\beta}^{\star}=\frac{\beta}{10}=0.09$$
$$\hat{\alpha}^{\star}=\alpha+\beta=1+0.9=1.9$$
$$\hat{\sigma}^{\star}=\sigma*10=20$$
$$SE(\hat{\beta}^{\star})=\frac{\hat{\sigma}^{\star}}{\sqrt{n\hat{Var}(x_j)}}$$
Since $\mathrm{x}^{\star}=10(\mathrm{x}-1)$, the variation of $x^{\star}$ will also equals the variation of $x$ times 100, so $$SE(\hat{\beta}^{\star})=SE(\hat{\beta})=0.03$$
Hence,
$$t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})=0.09/0.03=3$$


###5. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star\star})$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})$.


Let $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$,so $\mathrm{y}=(\mathrm{y}^{\star\star}/5)-2$. Hence we have the model:
$$\mathrm{y}=\alpha + \beta x + \mathrm{\epsilon}$$
$$(\mathrm{y}^{\star\star}/5)-2=\alpha + \beta x + \mathrm{\epsilon}$$
$$\mathrm{y}^{\star\star}=5\alpha+5\beta x+5\mathrm{\epsilon}+10$$
Since $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$, the variation of $y^{\star\star}$ will also equals the variation of $y$ times 25, so
$$\hat{\sigma}^{\star}=\sigma*5=10$$
$$\hat{\beta}^{\star\star}=5{\beta}=4.5$$
$$SE(\hat{\beta}^{\star})=\frac{\hat{\sigma}^{\star}}{\sqrt{n\hat{Var}(x_j)}}$$
$$SE(\hat{\beta}^{\star\star})=SE(\hat{\beta})=0.03$$
$$t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})=4.5/0.03=150$$



###6. In general, how are the hypothesis tests and confidence intervals for $\beta$ affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?

Confidence Interval:
$$(\hat{\beta_j}-t_{1-\alpha/2,n-p}SE(\hat{\beta_j}),\hat{\beta_j}+t_{1-\alpha/2,n-p}SE(\hat{\beta_j}))$$
Hence, if the scale of $\hat{\beta_j}$ is changed by linear transformations of $\mathrm{y}$ and $\mathrm{x}$, then the confidence interval will be moving up and down. Also, notice the $$SE(\hat{\beta_j})=\sqrt{\frac{1}{N}\sum_{i=1}^{n}(x_i-\overline{x})^2}$$
So the linear transformations which influence the value of x will also change the width of confidence interval.


		
# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

